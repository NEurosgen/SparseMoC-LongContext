"Идея проекта основана на работе
Mixture-of-Channels: Exploiting Sparse FFNs for Efficient LLMs Pre-Training and Inference
https://arxiv.org/abs/2511.09323
В настоящее время существует устойчивая тенденция
к увеличению длины контекста в LLM

Однако файнтюнинг моделей с длинным контекстом требует большого количества памяти. 
В указанной работе предложен метод спрасификации активаций во время обучения, что дает возможность сократить необходимый объем памяти для обучения.
Особенно большая оптимизация по памяти без просадки по качеству может быть при обучении с длинным контекстом.

Для выполнения проекта будет необходимо решить следующие задачи: 
1) Написать кернели для спарсификации активаций и быстрого умножения матриц при forward pass и backward pass
2) Обучить модель со спарсификации активаций и без спарсификации на длинном контексте (исходя из имеющихся ресурсов). 
Сравнить требуемый объем памяти и метрики моделей после обучения."